# -*- coding: utf-8 -*-
"""
MurasamePet API æœåŠ¡
æä¾›èŠå¤©ã€é—®ç­”å’Œè§†è§‰ç†è§£æ¥å£
"""

from fastapi import FastAPI, Request
from datetime import datetime
import uvicorn
import requests
import json
import torch
import platform
import sys
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from Murasame.utils import get_config

# ç¡®ä¿æ ‡å‡†è¾“å‡ºä½¿ç”¨ UTF-8 ç¼–ç ï¼Œé˜²æ­¢ä¸­æ–‡ä¹±ç 
if sys.stdout.encoding != 'utf-8':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ£€æµ‹å¹³å°å’Œå¼ºåˆ¶è¦æ±‚
IS_MACOS = platform.system() == "Darwin"

if IS_MACOS:
    # åœ¨ macOS ä¸Šå¼ºåˆ¶è¦æ±‚ MLX
    print("ğŸ æ£€æµ‹åˆ° macOS ç³»ç»Ÿï¼Œåˆå§‹åŒ– MLX å¼•æ“...")
    try:
        from mlx_lm.utils import load
        from mlx_lm.generate import generate
        ENGINE = "mlx"
        DEVICE = "mlx"  # MLX ä¼šè‡ªåŠ¨ä½¿ç”¨ Apple Silicon GPU (Metal)
        print("âœ… MLX å¼•æ“åŠ è½½æˆåŠŸ (Apple Silicon GPU åŠ é€Ÿ)")
    except ImportError as e:
        print(f"âŒ ä¸¥é‡é”™è¯¯ï¼šmacOS ç³»ç»Ÿéœ€è¦ MLX ä½†æœªæ‰¾åˆ°è¯¥åº“ï¼")
        print(f"å¯¼å…¥é”™è¯¯è¯¦æƒ…: {e}")
        print()
        print("ğŸ” è§£å†³æ–¹æ¡ˆï¼š")
        print("1. å®‰è£… MLX: pip install mlx-lm")
        print("2. æˆ–ç¡®ä¿æ‚¨ä½¿ç”¨çš„ Python ç¯å¢ƒæ”¯æŒ MLX")
        print()
        print("ğŸš¨ ç¨‹åºé€€å‡ºï¼šmacOS ç³»ç»Ÿå¿…é¡»ä½¿ç”¨ MLX ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
        exit(1)
else:
    # åœ¨é macOS ç³»ç»Ÿä¸Šä½¿ç”¨ PyTorch
    print("ğŸ–¥ï¸ æ£€æµ‹åˆ°é macOS ç³»ç»Ÿï¼Œåˆå§‹åŒ– PyTorch å¼•æ“...")
    ENGINE = "torch"
    # æ£€æµ‹è®¾å¤‡ä¼˜å…ˆçº§ï¼šMPS > CUDA > CPU
    if hasattr(torch, "backends") and hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        DEVICE = "mps"
        print("âœ… PyTorch å¼•æ“åŠ è½½æˆåŠŸ (ä½¿ç”¨ MPS åŠ é€Ÿ)")
    elif torch.cuda.is_available():
        DEVICE = "cuda"
        print("âœ… PyTorch å¼•æ“åŠ è½½æˆåŠŸ (ä½¿ç”¨ CUDA åŠ é€Ÿ)")
    else:
        DEVICE = "cpu"
        print("âš ï¸ PyTorch å¼•æ“åŠ è½½æˆåŠŸ (ä½¿ç”¨ CPUï¼Œæ€§èƒ½å¯èƒ½è¾ƒæ…¢)")

api = FastAPI()

adapter_path = "./models/Murasame"
max_seq_length = 2048


def load_model_and_tokenizer():
    print(f"ğŸ“‚ æ¨¡å‹åŠ è½½è·¯å¾„: {adapter_path}")
    print(f"âš™ï¸ æ¨ç†å¼•æ“: {ENGINE} | è®¡ç®—è®¾å¤‡: {DEVICE}")

    if IS_MACOS:
        # åœ¨ macOS ä¸Šä½¿ç”¨å·²åˆå¹¶çš„ MLX æ¨¡å‹
        print("ğŸ æ­£åœ¨åŠ è½½åˆå¹¶åçš„ MLX æ¨¡å‹ (Qwen3-14B-Murasame-Chat-MLX-Int4)...")

        # æ£€æŸ¥åˆå¹¶åçš„æ¨¡å‹æ˜¯å¦å­˜åœ¨
        # æ£€æŸ¥ MLX æ¨¡å‹å¿…éœ€æ–‡ä»¶ï¼ˆé…ç½®æ–‡ä»¶å’Œæ¨¡å‹æƒé‡ï¼‰
        required_static_files = ["tokenizer.json", "config.json"]
        missing_files = [f for f in required_static_files if not os.path.exists(os.path.join(adapter_path, f))]

        # æ£€æŸ¥æ¨¡å‹æƒé‡æ–‡ä»¶ï¼ˆå•ä¸ª model.safetensors æˆ–åˆ†ç‰‡ model-*.safetensorsï¼‰
        has_model_weights = False
        try:
            if os.path.exists(os.path.join(adapter_path, "model.safetensors")):
                has_model_weights = True
            else:
                if any(f.startswith("model-") and f.endswith(".safetensors") for f in os.listdir(adapter_path)):
                    has_model_weights = True
        except FileNotFoundError:
             # å¦‚æœ adapter_path ä¸å­˜åœ¨ï¼Œos.listdir ä¼šæŠ¥é”™
             pass

        if missing_files or not has_model_weights:
            if not has_model_weights:
                missing_files.append("model.safetensors (æˆ– model-*.safetensors)")
            
            print(f"âŒ ä¸¥é‡é”™è¯¯ï¼šåœ¨ {adapter_path} ä¸­ç¼ºå°‘ä»¥ä¸‹ MLX æ¨¡å‹æ–‡ä»¶: {', '.join(missing_files)}")
            print("ğŸ’¡ è¯·ç¡®ä¿å·²ä¸º macOS ä¸‹è½½äº†æ­£ç¡®çš„åˆå¹¶æ¨¡å‹ï¼Œè€Œä¸æ˜¯ Windows ä½¿ç”¨çš„ LoRA æ–‡ä»¶ã€‚")
            print("   - è¿è¡Œ 'python download.py' è„šæœ¬æ¥è·å–æ­£ç¡®çš„æ¨¡å‹ã€‚")
            exit(1)

        try:
            print("ğŸ”„ æ­£åœ¨ä»ç£ç›˜è¯»å–æ¨¡å‹æ–‡ä»¶...")
            # ç›´æ¥åŠ è½½åˆå¹¶åçš„å®Œæ•´æ¨¡å‹ï¼ˆä¸éœ€è¦å•ç‹¬çš„ base_model å’Œ adapterï¼‰
            model, tokenizer = load(adapter_path)
            print("âœ… åˆå¹¶ MLX æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            print(f"   ğŸ“ æ¨¡å‹è·¯å¾„: {adapter_path}")
            print(f"   ğŸ·ï¸ æ¨¡å‹ç±»å‹: Qwen3-14B + Murasame LoRA (å·²åˆå¹¶, Int4 é‡åŒ–)")
            print(f"   ğŸš€ å·²å¯ç”¨ Apple Silicon GPU åŠ é€Ÿ")

        except Exception as e:
            print(f"âŒ ä¸¥é‡é”™è¯¯ï¼šæ— æ³•åŠ è½½åˆå¹¶ MLX æ¨¡å‹ï¼")
            print(f"é”™è¯¯è¯¦æƒ…: {e}")
            print()
            print("ğŸ” å¯èƒ½çš„åŸå› ï¼š")
            print("1. æ¨¡å‹æ–‡ä»¶æŸåæˆ–ä¸å®Œæ•´")
            print("2. ä¸‹è½½çš„æ¨¡å‹ç‰ˆæœ¬ä¸ MLX ä¸å…¼å®¹")
            print("3. ç¼ºå°‘å¿…éœ€çš„ MLX ä¾èµ–")
            print()
            print("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
            print("1. é‡æ–°è¿è¡Œ download.py ç¡®ä¿åˆå¹¶æ¨¡å‹æ­£ç¡®ä¸‹è½½")
            print("2. æ£€æŸ¥ MLX å’Œ mlx-lm æ˜¯å¦æ­£ç¡®å®‰è£… (pip install mlx-lm)")
            print("3. éªŒè¯ ./models/Murasame ç›®å½•ä¸­çš„æ¨¡å‹æ–‡ä»¶")
            print()
            print("ğŸš¨ ç¨‹åºé€€å‡ºï¼šåº”ç”¨éœ€è¦åˆå¹¶ MLX æ¨¡å‹æ‰èƒ½è¿è¡Œ")
            exit(1)
    else:
        # åœ¨é macOS ç³»ç»Ÿä¸Šä½¿ç”¨ PyTorch (ä¿æŒåŸæœ‰é€»è¾‘)
        print("ğŸ”§ æ­£åœ¨åŠ è½½ PyTorch LoRA æ¨¡å‹...")

        try:
            print("ğŸ”„ æ­£åœ¨å‡†å¤‡åŸºç¡€æ¨¡å‹å’Œ LoRA é€‚é…å™¨...")
            adapter_config_path = os.path.join(adapter_path, "adapter_config.json")
            if not os.path.exists(adapter_config_path):
                print(f"âŒ ä¸¥é‡é”™è¯¯ï¼šæœªæ‰¾åˆ°é€‚é…å™¨é…ç½®æ–‡ä»¶ {adapter_config_path}")
                print("ğŸ’¡ è¯·è¿è¡Œ download.py ä»¥ä¸‹è½½åŸºç¡€æ¨¡å‹ä¸ LoRA é€‚é…å™¨")
                exit(1)

            with open(adapter_config_path, "r", encoding="utf-8") as f:
                adapter_config = json.load(f)

            base_model_path = adapter_config.get("base_model_name_or_path")
            if not base_model_path:
                print("âŒ ä¸¥é‡é”™è¯¯ï¼šé€‚é…å™¨é…ç½®ç¼ºå°‘ base_model_name_or_path")
                exit(1)
            if not os.path.exists(base_model_path):
                print(f"âŒ ä¸¥é‡é”™è¯¯ï¼šåŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {base_model_path}")
                print("ğŸ’¡ è¯·ç¡®è®¤ Qwen3-14B æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½å¹¶ä¸ adapter_config.json ä¸­çš„è·¯å¾„ä¸€è‡´")
                exit(1)

            torch_dtype = torch.float16 if DEVICE == "cuda" else torch.float32
            device_map = "balanced" if DEVICE == "cuda" else "cpu"
            if DEVICE == "cpu":
                print("âš ï¸  è­¦å‘Š: åœ¨ CPU ä¸ŠåŠ è½½ 14B æ¨¡å‹éœ€è¦å¤§é‡å†…å­˜ (é€šå¸¸ > 32GB)ï¼Œè¯·ç¡®ä¿å¯ç”¨å†…å­˜å……è¶³ã€‚")

            print(f"ğŸ“¦ æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                torch_dtype=torch_dtype,
                device_map=device_map,
                trust_remote_code=True,
            )

            print(f"ğŸ¯ æ­£åœ¨åº”ç”¨ LoRA é€‚é…å™¨: {adapter_path}")
            model = PeftModel.from_pretrained(
                base_model,
                adapter_path,
                device_map=device_map,
            )

            # if DEVICE in ("cpu", "cuda"):
            #     model = model.to(DEVICE)

            model.eval()

            tokenizer = AutoTokenizer.from_pretrained(
                base_model_path,
                trust_remote_code=True,
            )
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            tokenizer.padding_side = "left"

            print("âœ… LoRA æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            print(f"   ğŸ“ åŸºç¡€æ¨¡å‹: {base_model_path}")
            print(f"   ğŸ“ é€‚é…å™¨: {adapter_path}")
            print(f"   ğŸ·ï¸ æ¨ç†è®¾å¤‡: {DEVICE}")
        except Exception as e:
            print(f"âŒ ä¸¥é‡é”™è¯¯ï¼šæ— æ³•åŠ è½½ PyTorch LoRA æ¨¡å‹ï¼")
            print(f"é”™è¯¯è¯¦æƒ…: {e}")
            print()
            print("ğŸ” å¯èƒ½çš„åŸå› ï¼š")
            print("1. LoRA æ–‡ä»¶æŸåæˆ–ä¸å®Œæ•´")
            print("2. ç¼ºå°‘å¿…éœ€çš„ PyTorch ä¾èµ–")
            print()
            print("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
            print("é‡æ–°è¿è¡Œ download.py ç¡®ä¿ LoRA æ–‡ä»¶æ­£ç¡®ä¸‹è½½")
            print()
            print("ğŸš¨ ç¨‹åºé€€å‡ºï¼šåº”ç”¨éœ€è¦ LoRA æ¨¡å‹æ‰èƒ½è¿è¡Œ")
            exit(1)

    return model, tokenizer


# è¾…åŠ©å‡½æ•°ï¼šè·å–å½“å‰æ—¶é—´
def get_current_time():
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


# è¾…åŠ©å‡½æ•°ï¼šè®°å½•è¯·æ±‚æ—¥å¿—
def log_request(prompt):
    print(f'ğŸ“¥ [{get_current_time()}] æ”¶åˆ°ç”¨æˆ·è¯·æ±‚: {prompt}')


# è¾…åŠ©å‡½æ•°ï¼šè®°å½•å“åº”æ—¥å¿—
def log_response(response):
    print(f'ğŸ“¤ [{get_current_time()}] ç”Ÿæˆæœ€ç»ˆå›å¤: {response}')


# è¾…åŠ©å‡½æ•°ï¼šè§£æè¯·æ±‚
def parse_request(json_post_list):
    prompt = json_post_list.get('prompt')
    history = json_post_list.get('history')
    return prompt, history


# è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºæ ‡å‡†å“åº”
def create_response(response_text, history, status=200):
    time = get_current_time()
    return {
        "response": response_text,
        "history": history,
        "status": status,
        "time": time
    }


# MLX ä¸éœ€è¦æ‰‹åŠ¨åƒåœ¾å›æ”¶


def call_openrouter_api(config, api_key, model, messages, image_url=None, max_tokens=2048):
    """è°ƒç”¨ OpenRouter API"""
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://murasame-pet.local",
        "X-Title": "MurasamePet"
    }

    # å¤„ç†å›¾åƒè¾“å…¥ - æŒ‰ç…§ OpenRouter å®˜æ–¹æ–‡æ¡£æ ¼å¼
    if image_url:
        # å¦‚æœæœ‰å›¾åƒï¼Œå°†æœ€åä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯ä¿®æ”¹ä¸ºåŒ…å«å›¾åƒ
        for message in reversed(messages):
            if message['role'] == 'user':
                if isinstance(message['content'], str):
                    # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå®˜æ–¹æ–‡æ¡£è¦æ±‚çš„æ•°ç»„æ ¼å¼
                    message['content'] = [
                        {"type": "text", "text": message['content']},
                        {"type": "image_url", "image_url": {"url": image_url}}
                    ]
                elif isinstance(message['content'], list):
                    # å¦‚æœå·²ç»æ˜¯æ•°ç»„æ ¼å¼ï¼Œç›´æ¥æ·»åŠ å›¾åƒ
                    message['content'].append({"type": "image_url", "image_url": {"url": image_url}})
                break

    data = {
        "model": model,
        "messages": messages,
        "temperature": 0.7,
        "max_tokens": max_tokens
    }

    # ä»é…ç½®ä¸­è·å– OpenRouter åœ°å€ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
    endpoint_url = config.get('endpoints', {}).get('openrouter', "https://openrouter.ai/api/v1/chat/completions")
    response = requests.post(endpoint_url, headers=headers, json=data)
    response.raise_for_status()
    return response.json()


@api.post("/chat")
async def create_chat(request: Request):
    json_post_list = await request.json()
    prompt, history = parse_request(json_post_list)
    log_request(prompt)
    history = history + [{'role': 'user', 'content': prompt}]

    # ä½¿ç”¨ MLX è¿›è¡Œæ¨ç†
    print(f"ğŸ’¬ ä½¿ç”¨ {ENGINE.upper()} å¼•æ“è¿›è¡Œæ¨ç†...")
    print(f"ğŸ“Š æœ€å¤§ç”Ÿæˆé•¿åº¦: {json_post_list.get('max_new_tokens', 2048)} tokens")
    
    text = tokenizer.apply_chat_template(
        history,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False,
    )
    print("âœ… èŠå¤©æ¨¡æ¿åº”ç”¨å®Œæˆ")

    max_new_tokens = int(json_post_list.get('max_new_tokens', 2048))
    max_new_tokens = max(1, max_new_tokens)
    temperature = float(json_post_list.get('temperature', 0.7))
    top_p = float(json_post_list.get('top_p', 0.9))
    top_p = max(0.01, min(top_p, 1.0))

    # æ¨ç†
    print("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›å¤...")
    if ENGINE == "mlx":
        response = generate(
            model, tokenizer,
            prompt=text,
            max_tokens=max_new_tokens,
            verbose=False
        )
        reply = response.strip()
    else:
        encoded = tokenizer(
            text,
            return_tensors="pt",
        )
        encoded = {k: v.to(DEVICE) for k, v in encoded.items()}
        generation_kwargs = {
            "max_new_tokens": max_new_tokens,
            "do_sample": True,
            "temperature": max(0.01, temperature),
            "top_p": top_p,
            "eos_token_id": tokenizer.eos_token_id,
            "pad_token_id": tokenizer.eos_token_id,
        }
        with torch.no_grad():
            generated = model.generate(
                **encoded,
                **generation_kwargs,
            )
        generated_tokens = generated[0, encoded["input_ids"].shape[-1]:]
        reply = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()

    print(f"âœ… å›å¤ç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(reply)} å­—ç¬¦)")

    history.append({"role": "assistant", "content": reply})

    log_response(reply)
    return create_response(reply, history)


# @api.post("/qwen3")
# async def create_qwen3_chat(request: Request):
#     json_post_list = await request.json()
#     prompt, history = parse_request(json_post_list)
#     role = json_post_list.get('role', 'user')
#     log_request(prompt)
#     if prompt != "":
#         history = history + [{'role': role, 'content': prompt}]

#     config = get_config()
#     api_key = config.get('openrouter_api_key', '')
#     endpoint_url = config.get('server', {}).get('qwen3', '')

#     # ä»…å½“ endpoint æŒ‡å‘ openrouter ä¸” API key å­˜åœ¨æ—¶ï¼Œæ‰ä½¿ç”¨ OpenRouter
#     if "openrouter.ai" in endpoint_url and api_key.strip():
#         print(f"ğŸŒ æ£€æµ‹åˆ° qwen3 endpoint æŒ‡å‘ OpenRouterï¼Œä½¿ç”¨ API Key è¿›è¡Œè°ƒç”¨...")
#         try:
#             result = call_openrouter_api(
#                 config,
#                 api_key,
#                 "qwen/qwen3-235b-a22b",
#                 history,
#                 max_tokens=4096
#             )
#             final_response = result['choices'][0]['message']['content']
#             print("âœ… OpenRouter API è°ƒç”¨æˆåŠŸ")
#         except Exception as e:
#             error_msg = f"OpenRouter API é”™è¯¯: {str(e)}"
#             print(f"âŒ {error_msg}")
#             log_response(error_msg)
#             return create_response(error_msg, history, status=500)
#     else:
#         # ä½¿ç”¨æœ¬åœ°ç«¯ç‚¹ (Ollama æˆ–å…¶ä»–)
#         print(f"ğŸ  ä½¿ç”¨æœ¬åœ°ç«¯ç‚¹ ({endpoint_url}) è¿›è¡Œè°ƒç”¨...")
#         response = None
#         try:
#             response = requests.post(
#                 f"{endpoint_url}/api/chat",
#                 json={"model": "qwen3:14b", "messages": history,
#                       "stream": False, "options": {"keep_alive": -1}},
#             )
#             response.raise_for_status() # æ£€æŸ¥ HTTP é”™è¯¯
#             final_response = response.json()['message']['content']
#             print("âœ… æœ¬åœ° API è°ƒç”¨æˆåŠŸ")
#         except requests.exceptions.RequestException as e:
#             print(f"âŒ è°ƒç”¨æœ¬åœ° API æ—¶å‡ºé”™: {e}")
#             if response is not None:
#                 print(f"å“åº”çŠ¶æ€: {response.status_code}")
#                 print(f"å“åº”å†…å®¹: {response.text[:500]}")
#             raise

#     history = history + [{'role': 'assistant', 'content': final_response}]
#     log_response(final_response)
#     return create_response(final_response, history)

@api.post("/qwen3")
async def create_qwen3_chat(request: Request):
    json_post_list = await request.json()
    prompt, history = parse_request(json_post_list)  # è§£æè¯·æ±‚ä¸­çš„promptå’Œhistory
    role = json_post_list.get('role', 'user')
    log_request(prompt)

    # æ„å»ºå¯¹è¯å†å²ï¼ˆç”¨æˆ·è¾“å…¥ + å†å²è®°å½•ï¼‰
    if prompt != "":
        history = history + [{'role': role, 'content': prompt}]

    # é…ç½®æ¨ç†å‚æ•°ï¼ˆä½¿ç”¨è¯·æ±‚ä¸­çš„å‚æ•°æˆ–é»˜è®¤å€¼ï¼‰
    max_new_tokens = int(json_post_list.get('max_new_tokens', 2048))
    max_new_tokens = max(1, max_new_tokens)
    temperature = float(json_post_list.get('temperature', 0.7))
    top_p = float(json_post_list.get('top_p', 0.9))
    top_p = max(0.01, min(top_p, 1.0))

    # åº”ç”¨èŠå¤©æ¨¡æ¿ï¼ˆé€‚é… Qwen3 çš„å¯¹è¯æ ¼å¼ï¼‰
    print(f"ğŸ’¬ ä½¿ç”¨ {ENGINE.upper()} å¼•æ“è¿›è¡Œ Qwen3 æ¨ç†...")
    print(f"ğŸ“Š æœ€å¤§ç”Ÿæˆé•¿åº¦: {max_new_tokens} tokens")
    text = tokenizer.apply_chat_template(
        history,
        tokenize=False,
        add_generation_prompt=True,  # è‡ªåŠ¨æ·»åŠ  assistant ç”Ÿæˆå‰ç¼€
        enable_thinking=False,
    )
    print("âœ… Qwen3 èŠå¤©æ¨¡æ¿åº”ç”¨å®Œæˆ")

    # è°ƒç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆå›å¤
    print("ğŸ¤– æ­£åœ¨ç”Ÿæˆ Qwen3 å›å¤...")
    try:
        if ENGINE == "mlx":
            # MLX å¼•æ“æ¨ç†ï¼ˆApple Siliconï¼‰
            response = generate(
                model, tokenizer,
                prompt=text,
                max_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                verbose=False
            )
            reply = response.strip()
        else:
            # PyTorch å¼•æ“æ¨ç†ï¼ˆCUDA/CPUï¼‰
            encoded = tokenizer(
                text,
                return_tensors="pt",
            )
            encoded = {k: v.to(DEVICE) for k, v in encoded.items()}
            generation_kwargs = {
                "max_new_tokens": max_new_tokens,
                "do_sample": True,
                "temperature": max(0.01, temperature),
                "top_p": top_p,
                "eos_token_id": tokenizer.eos_token_id,
                "pad_token_id": tokenizer.eos_token_id,
            }
            with torch.no_grad():
                generated = model.generate(
                    **encoded,** generation_kwargs,
                )
            generated_tokens = generated[0, encoded["input_ids"].shape[-1]:]
            reply = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()

        print(f"âœ… Qwen3 å›å¤ç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(reply)} å­—ç¬¦)")
    except Exception as e:
        error_msg = f"æœ¬åœ° Qwen3 æ¨¡å‹æ¨ç†å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        return create_response(error_msg, history, status=500)

    # æ›´æ–°å¯¹è¯å†å²å¹¶è¿”å›å“åº”
    history.append({"role": "assistant", "content": reply})
    log_response(reply)
    return create_response(reply, history)

@api.post("/qwenvl")
async def create_qwenvl_chat(request: Request):
    json_post_list = await request.json()
    prompt, history = parse_request(json_post_list)
    log_request(prompt)

    if "image" in json_post_list:
        image_url = json_post_list.get('image')
        print(f"ğŸ–¼ï¸ æ£€æµ‹åˆ°å›¾åƒè¾“å…¥: {image_url[:100]}...")
        history = history + [{'role': 'user', 'content': prompt, 'images': [image_url]}]
    else:
        print("ğŸ“ çº¯æ–‡æœ¬æ¨¡å¼ï¼ˆæ— å›¾åƒè¾“å…¥ï¼‰")
        history = history + [{'role': 'user', 'content': prompt}]

    config = get_config()
    api_key = config.get('openrouter_api_key', '')
    endpoint_url = config.get('server', {}).get('qwenvl', '')
    image_url_for_api = json_post_list.get('image') if "image" in json_post_list else None

    # ä»…å½“ endpoint æŒ‡å‘ openrouter ä¸” API key å­˜åœ¨æ—¶ï¼Œæ‰ä½¿ç”¨ OpenRouter
    if "openrouter.ai" in endpoint_url and api_key.strip():
        print(f"ğŸŒ æ£€æµ‹åˆ° qwenvl endpoint æŒ‡å‘ OpenRouterï¼Œä½¿ç”¨ API Key è¿›è¡Œè°ƒç”¨...")
        try:
            result = call_openrouter_api(
                config,
                api_key,
                "qwen/qwen-2.5-vl-7b-instruct",
                history,
                image_url=image_url_for_api
            )
            final_response = result['choices'][0]['message']['content']
            print("âœ… OpenRouter è§†è§‰ API è°ƒç”¨æˆåŠŸ")
        except Exception as e:
            error_msg = f"OpenRouter API é”™è¯¯: {str(e)}"
            print(f"âŒ {error_msg}")
            log_response(error_msg)
            return create_response(error_msg, history, status=500)
    else:
        # ä½¿ç”¨æœ¬åœ°ç«¯ç‚¹ (Ollama æˆ–å…¶ä»–)
        print(f"ğŸ  ä½¿ç”¨æœ¬åœ°ç«¯ç‚¹ ({endpoint_url}) è¿›è¡Œè°ƒç”¨...")
        try:
            response = requests.post(
                f"{endpoint_url}/api/chat",
                json={"model": "qwen2.5vl:7b", "messages": history,
                      "stream": False, "options": {"keep_alive": -1}},
            )
            response.raise_for_status()
            final_response = response.json()['message']['content']
            print("âœ… æœ¬åœ°è§†è§‰ API è°ƒç”¨æˆåŠŸ")
        except requests.exceptions.RequestException as e:
            print(f"âŒ è°ƒç”¨æœ¬åœ°è§†è§‰ API æ—¶å‡ºé”™: {e}")
            raise

    history = history + [{'role': 'assistant', 'content': final_response}]
    log_response(final_response)
    return create_response(final_response, history)

if __name__ == '__main__':
    print("=" * 60)
    print("ğŸš€ MurasamePet API æœåŠ¡å¯åŠ¨ä¸­...")
    print("=" * 60)
    
    model, tokenizer = load_model_and_tokenizer()
    
    print("=" * 60)
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¯åŠ¨ FastAPI æœåŠ¡å™¨...")
    print(f"ğŸŒ æœåŠ¡åœ°å€: http://0.0.0.0:8565")
    print(f"ğŸ“¡ å¯ç”¨ç«¯ç‚¹:")
    print(f"   - POST /chat    (ä¸»å¯¹è¯æ¥å£ - Murasame)")
    print(f"   - POST /qwen3   (é€šç”¨é—®ç­”æ¥å£ - Qwen3)")
    print(f"   - POST /qwenvl  (è§†è§‰ç†è§£æ¥å£ - Qwen-VL)")
    print("=" * 60)
    
    uvicorn.run(api, host='0.0.0.0', port=8565, workers=1)
